/******************************************************************************
 * ds_cnn_inference.c
 *
 * Full DS–CNN inference implementation for int8–quantized keyword spotting.
 * 
 * This code is intended to run on a bare–metal Cortex–M55 with MVE support.
 * It uses CMSIS–NN (and/or CMSIS–DSP) for optimized kernels. The weights,
 * biases, and quantization parameters are generated by generate_ds_cnn_data.py,
 * and stored in ds_cnn_data.c/ds_cnn_data.h.
 *
 * The network implemented here is:
 *   Input (assumed shape: 1 x 49 x 10 x 1) -->
 *     conv1 (normal convolution) + BN + ReLU -->
 *     4 blocks each: { depthwise conv + BN + ReLU, then pointwise conv + BN + ReLU } -->
 *     Global average pooling -->
 *     Fully connected layer (with per–channel quantization) -->
 *     Softmax.
 *
 * All dropout layers are bypassed.
 *
 * NOTE: The layer dimensions, strides, and BN parameters below are assumed.
 * You must adjust the macros and parameter structures to exactly match your
 * Keras DS–CNN model.
 *
 * The naming of the arrays (e.g. conv1_weights, conv1_bias, conv1_multiplier,
 * conv1_shift, conv1_bn_multiplier, conv1_bn_shift, block1_dw_weights, etc.)
 * is assumed to match what generate_ds_cnn_data.py produces.
 *
 * To unit–test this code, a test input (test_input) and expected output 
 * (expected_output) are compiled in. The main() function compares the result.
 *
 ******************************************************************************/

#include <stdio.h>
#include <stdlib.h>
#include <stdint.h>
#include <string.h>

/* CMSIS–NN functions and types.
   (Assume that arm_nnfunctions.h from CMSIS–NN is in your include path.) */
#include "arm_nnfunctions.h"

/* Generated data from the TFLite flatbuffer (weights, biases, quantization, test input,
   and expected output). This file is produced by generate_ds_cnn_data.py. */
#include "ds_cnn_data.h"

/*----------------------------------------------------------------------------
  Define layer dimensions and parameters.
  (Adjust these macros so that they exactly match your model.)
 *---------------------------------------------------------------------------*/

#define BATCH_SIZE       1

/* Assume the input is a 49 x 10 mel spectrogram with 1 channel */
#define INPUT_H          49
#define INPUT_W          10
#define INPUT_C          1

/* --- Conv1 parameters (first convolution) --- */
#define CONV1_OUT_C      64
#define CONV1_KH         10
#define CONV1_KW         4
#define CONV1_STRIDE_H   2
#define CONV1_STRIDE_W   2
/* For “SAME” padding, we assume: */
#define CONV1_PAD_H      ((CONV1_KH - 1) / 2)
#define CONV1_PAD_W      ((CONV1_KW - 1) / 2)
/* Output dimensions computed as ceiling(INPUT/(stride)) */
#define CONV1_OUT_H      (((INPUT_H + CONV1_STRIDE_H - 1)) / (CONV1_STRIDE_H))
#define CONV1_OUT_W      (((INPUT_W + CONV1_STRIDE_W - 1)) / (CONV1_STRIDE_W))

/* --- DS–CNN Block parameters (each block has a depthwise conv and a pointwise conv) --- */
/* Assume kernel size 3x3 for depthwise, with stride 1 and SAME padding. */
#define DW_KH            3
#define DW_KW            3
#define DW_STRIDE_H      1
#define DW_STRIDE_W      1
#define DW_PAD_H         ((DW_KH - 1) / 2)
#define DW_PAD_W         ((DW_KW - 1) / 2)
/* Assume pointwise conv is 1x1 and does not change spatial dims. */
#define PW_KH            1
#define PW_KW            1

/* For all blocks, we assume the spatial dimensions remain the same as conv1_out */
#define BLOCK_OUT_H      CONV1_OUT_H
#define BLOCK_OUT_W      CONV1_OUT_W
#define BLOCK_OUT_C      CONV1_OUT_C  /* In DS–CNN the channel count is often preserved */

/* --- Fully connected layer --- */
#define FC_OUT           12  /* number of keyword classes */

/*----------------------------------------------------------------------------
  Helper functions
 *---------------------------------------------------------------------------*/

/* A simple per–element batch normalization followed by ReLU.
   Here we assume that the BN parameters are given per–tensor (or the first element is used).
   In a complete implementation you might loop over channels and use per–channel values.
 */
static void apply_batch_norm_relu_s8(int8_t *data, int32_t size,
                                     const int32_t *bn_multiplier, const int32_t *bn_shift)
{
    for (int i = 0; i < size; i++) {
        int32_t x = data[i];
        /* Apply BN: y = x * multiplier + shift.
           (In practice, the quantization procedure may include a right shift; here we assume it is pre–computed.)
         */
        x = x * bn_multiplier[0] + bn_shift[0];
        /* Apply ReLU: clamp negatives to 0, and saturate to 127 */
        if (x < 0)
            x = 0;
        if (x > 127)
            x = 127;
        data[i] = (int8_t)x;
    }
}

/*----------------------------------------------------------------------------
  Layer wrapper functions
 *---------------------------------------------------------------------------*/

/* Standard 2D convolution with BN and ReLU.
   Uses CMSIS–NN’s arm_convolve_wrapper_s8.
 */
static arm_cmsis_nn_status conv2d_bn_relu(const cmsis_nn_conv_params *conv_params,
                                            const cmsis_nn_per_tensor_quant_params *quant_params,
                                            const cmsis_nn_dims *input_dims,
                                            const int8_t *input_data,
                                            const cmsis_nn_dims *filter_dims,
                                            const int8_t *filter_data,
                                            const cmsis_nn_dims *bias_dims,
                                            const int32_t *bias_data,
                                            const cmsis_nn_dims *output_dims,
                                            int8_t *output_data,
                                            const int32_t *bn_multiplier, const int32_t *bn_shift)
{
    /* Allocate a temporary buffer for the convolution output.
       (In production code, this buffer would be allocated from scratch memory.)
    */
    int32_t out_size = output_dims->h * output_dims->w * output_dims->c;
    int8_t *conv_out = (int8_t *)malloc(out_size * sizeof(int8_t));
    if (conv_out == NULL)
        return ARM_CMSIS_NN_ARG_ERROR;
    memset(conv_out, 0, out_size * sizeof(int8_t));

    arm_cmsis_nn_status status;
    status = arm_convolve_wrapper_s8(NULL, conv_params, quant_params, input_dims, input_data,
                                       filter_dims, filter_data, bias_dims, bias_data,
                                       output_dims, conv_out);
    if (status != ARM_CMSIS_NN_SUCCESS) {
        free(conv_out);
        return status;
    }
    /* Apply BN and ReLU */
    apply_batch_norm_relu_s8(conv_out, out_size, bn_multiplier, bn_shift);
    memcpy(output_data, conv_out, out_size * sizeof(int8_t));
    free(conv_out);
    return ARM_CMSIS_NN_SUCCESS;
}

/* Depthwise convolution with BN and ReLU.
   Uses arm_depthwise_conv_wrapper_s8.
 */
static arm_cmsis_nn_status depthwise_conv2d_bn_relu(const cmsis_nn_dw_conv_params *dw_conv_params,
                                                      const cmsis_nn_per_tensor_quant_params *quant_params,
                                                      const cmsis_nn_dims *input_dims,
                                                      const int8_t *input_data,
                                                      const cmsis_nn_dims *filter_dims,
                                                      const int8_t *filter_data,
                                                      const cmsis_nn_dims *bias_dims,
                                                      const int32_t *bias_data,
                                                      const cmsis_nn_dims *output_dims,
                                                      int8_t *output_data,
                                                      const int32_t *bn_multiplier, const int32_t *bn_shift)
{
    int32_t out_size = output_dims->h * output_dims->w * output_dims->c;
    int8_t *dw_out = (int8_t *)malloc(out_size * sizeof(int8_t));
    if (dw_out == NULL)
        return ARM_CMSIS_NN_ARG_ERROR;
    memset(dw_out, 0, out_size * sizeof(int8_t));

    arm_cmsis_nn_status status;
    status = arm_depthwise_conv_wrapper_s8(NULL, dw_conv_params, quant_params, input_dims, input_data,
                                             filter_dims, filter_data, bias_dims, bias_data,
                                             output_dims, dw_out);
    if (status != ARM_CMSIS_NN_SUCCESS) {
        free(dw_out);
        return status;
    }
    apply_batch_norm_relu_s8(dw_out, out_size, bn_multiplier, bn_shift);
    memcpy(output_data, dw_out, out_size * sizeof(int8_t));
    free(dw_out);
    return ARM_CMSIS_NN_SUCCESS;
}

/* Pointwise (1x1) convolution with BN and ReLU.
   Uses arm_convolve_wrapper_s8.
 */
static arm_cmsis_nn_status pointwise_conv2d_bn_relu(const cmsis_nn_conv_params *pw_params,
                                                      const cmsis_nn_per_tensor_quant_params *quant_params,
                                                      const cmsis_nn_dims *input_dims,
                                                      const int8_t *input_data,
                                                      const cmsis_nn_dims *filter_dims,
                                                      const int8_t *filter_data,
                                                      const cmsis_nn_dims *bias_dims,
                                                      const int32_t *bias_data,
                                                      const cmsis_nn_dims *output_dims,
                                                      int8_t *output_data,
                                                      const int32_t *bn_multiplier, const int32_t *bn_shift)
{
    int32_t out_size = output_dims->h * output_dims->w * output_dims->c;
    int8_t *pw_out = (int8_t *)malloc(out_size * sizeof(int8_t));
    if (pw_out == NULL)
        return ARM_CMSIS_NN_ARG_ERROR;
    memset(pw_out, 0, out_size * sizeof(int8_t));

    arm_cmsis_nn_status status;
    status = arm_convolve_wrapper_s8(NULL, pw_params, quant_params, input_dims, input_data,
                                       filter_dims, filter_data, bias_dims, bias_data,
                                       output_dims, pw_out);
    if (status != ARM_CMSIS_NN_SUCCESS) {
        free(pw_out);
        return status;
    }
    apply_batch_norm_relu_s8(pw_out, out_size, bn_multiplier, bn_shift);
    memcpy(output_data, pw_out, out_size * sizeof(int8_t));
    free(pw_out);
    return ARM_CMSIS_NN_SUCCESS;
}

/* Global average pooling over the spatial dimensions.
   For each channel, compute the average over (height*width).
 */
static void global_average_pooling_s8(const cmsis_nn_dims *input_dims,
                                      const int8_t *input_data, int8_t *output_data)
{
    int h = input_dims->h;
    int w = input_dims->w;
    int c = input_dims->c;
    for (int ch = 0; ch < c; ch++) {
        int32_t sum = 0;
        for (int i = 0; i < h * w; i++) {
            sum += input_data[i * c + ch];
        }
        int avg = sum / (h * w);
        if (avg < -128) avg = -128;
        if (avg > 127) avg = 127;
        output_data[ch] = (int8_t)avg;
    }
}

/* Fully connected layer using per–channel quantization.
   Uses arm_fully_connected_wrapper_s8 from CMSIS–NN.
 */
static arm_cmsis_nn_status fully_connected_layer(const cmsis_nn_per_channel_quant_params *fc_quant_params,
                                                   const cmsis_nn_dims *input_dims,
                                                   const int8_t *input_data,
                                                   const cmsis_nn_dims *filter_dims,
                                                   const int8_t *filter_data,
                                                   const cmsis_nn_dims *bias_dims,
                                                   const int32_t *bias_data,
                                                   const cmsis_nn_dims *output_dims,
                                                   int8_t *output_data)
{
    return arm_fully_connected_wrapper_s8(NULL, fc_quant_params, input_dims, input_data,
                                            filter_dims, filter_data, bias_dims, bias_data,
                                            output_dims, output_data);
}

/* Softmax using CMSIS–NN. */
static void softmax_layer_s8(const int8_t *input, int num_rows, int row_size, int8_t *output)
{
    /* The quantization multiplier, shift and diff_min must be chosen according to the output scale.
       Here we simply use dummy values. Adjust as needed. */
    int32_t mult = 1;
    int32_t shift = 0;
    int32_t diff_min = -128;
    arm_softmax_s8(input, num_rows, row_size, mult, shift, diff_min, output);
}

/*----------------------------------------------------------------------------
  The DS–CNN inference function.
 *---------------------------------------------------------------------------*/
arm_cmsis_nn_status ds_cnn_inference(const int8_t *input, int8_t *output)
{
    arm_cmsis_nn_status status;

    /* === Layer 1: conv1 === */
    cmsis_nn_dims in_dims = { .n = BATCH_SIZE, .h = INPUT_H, .w = INPUT_W, .c = INPUT_C };
    cmsis_nn_dims conv1_filter_dims = { .n = CONV1_OUT_C, .h = CONV1_KH, .w = CONV1_KW, .c = INPUT_C };
    cmsis_nn_dims conv1_bias_dims   = { .n = 0, .h = 0, .w = 0, .c = CONV1_OUT_C };
    cmsis_nn_dims conv1_out_dims    = { .n = BATCH_SIZE, .h = CONV1_OUT_H, .w = CONV1_OUT_W, .c = CONV1_OUT_C };

    cmsis_nn_conv_params conv1_params = {
        .stride = { .h = CONV1_STRIDE_H, .w = CONV1_STRIDE_W },
        .padding = { .h = CONV1_PAD_H, .w = CONV1_PAD_W },
        .input_offset = 0,
        .output_offset = 0,
        .activation = { .min = 0, .max = 127 }
    };
    /* Use conv1_multiplier and conv1_shift (per–tensor quantization) extracted from the flatbuffer */
    cmsis_nn_per_tensor_quant_params conv1_quant = {
        .multiplier = conv1_multiplier[0],
        .shift = conv1_shift[0]
    };

    /* Allocate temporary buffer for conv1 output */
    int32_t conv1_out_size = conv1_out_dims.h * conv1_out_dims.w * conv1_out_dims.c;
    int8_t *conv1_output = (int8_t *)malloc(conv1_out_size * sizeof(int8_t));
    if (conv1_output == NULL)
        return ARM_CMSIS_NN_ARG_ERROR;

    status = conv2d_bn_relu(&conv1_params, &conv1_quant, &in_dims, input,
                            &conv1_filter_dims, conv1_weights, &conv1_bias_dims, conv1_bias,
                            &conv1_out_dims, conv1_output,
                            conv1_bn_multiplier, conv1_bn_shift);
    if (status != ARM_CMSIS_NN_SUCCESS) {
        free(conv1_output);
        return status;
    }

    /* === DS–CNN Blocks (4 blocks, each with depthwise conv then pointwise conv) === */
    /* For each block, we assume the input and output dims remain the same as conv1_out_dims. */
    cmsis_nn_dims block_in_dims = conv1_out_dims;
    cmsis_nn_dims block_out_dims = conv1_out_dims;
    /* Depthwise conv parameters: kernel 3x3, stride 1, SAME padding */
    cmsis_nn_dw_conv_params dw_params = {
        .stride = { .h = DW_STRIDE_H, .w = DW_STRIDE_W },
        .padding = { .h = DW_PAD_H, .w = DW_PAD_W },
        .input_offset = 0,
        .output_offset = 0,
        .activation = { .min = 0, .max = 127 }
    };
    cmsis_nn_dims dw_filter_dims = { .n = 0, .h = DW_KH, .w = DW_KW, .c = BLOCK_OUT_C };
    cmsis_nn_dims dw_bias_dims   = { .n = 0, .h = 0, .w = 0, .c = BLOCK_OUT_C };

    /* Pointwise conv parameters: 1x1 conv, stride 1, no padding */
    cmsis_nn_conv_params pw_params = {
        .stride = { .h = 1, .w = 1 },
        .padding = { .h = 0, .w = 0 },
        .input_offset = 0,
        .output_offset = 0,
        .activation = { .min = 0, .max = 127 }
    };
    cmsis_nn_dims pw_filter_dims = { .n = BLOCK_OUT_C, .h = PW_KH, .w = PW_KW, .c = BLOCK_OUT_C };
    cmsis_nn_dims pw_bias_dims   = { .n = 0, .h = 0, .w = 0, .c = BLOCK_OUT_C };

    /* Quantization parameters for depthwise and pointwise conv layers */
    cmsis_nn_per_tensor_quant_params dw_quant = {
        .multiplier = dw_conv_multiplier[0],
        .shift = dw_conv_shift[0]
    };
    cmsis_nn_per_tensor_quant_params pw_quant = {
        .multiplier = pw_conv_multiplier[0],
        .shift = pw_conv_shift[0]
    };

    /* For intermediate results, we alternate between two buffers.
       Here we allocate one temporary buffer (block_buffer) of size equal to conv1_output.
    */
    int32_t block_buf_size = conv1_out_size;
    int8_t *block_buffer = (int8_t *)malloc(block_buf_size * sizeof(int8_t));
    if (block_buffer == NULL) {
        free(conv1_output);
        return ARM_CMSIS_NN_ARG_ERROR;
    }

    /* --- Block 1 --- */
    status = depthwise_conv2d_bn_relu(&dw_params, &dw_quant, &block_in_dims, conv1_output,
                                      &dw_filter_dims, block1_dw_weights, &dw_bias_dims, block1_dw_bias,
                                      &block_out_dims, block_buffer,
                                      block1_dw_bn_multiplier, block1_dw_bn_shift);
    if (status != ARM_CMSIS_NN_SUCCESS) goto cleanup;
    status = pointwise_conv2d_bn_relu(&pw_params, &pw_quant, &block_in_dims, block_buffer,
                                      &pw_filter_dims, block1_pw_weights, &pw_bias_dims, block1_pw_bias,
                                      &block_out_dims, conv1_output,
                                      block1_pw_bn_multiplier, block1_pw_bn_shift);
    if (status != ARM_CMSIS_NN_SUCCESS) goto cleanup;

    /* --- Block 2 --- */
    status = depthwise_conv2d_bn_relu(&dw_params, &dw_quant, &block_in_dims, conv1_output,
                                      &dw_filter_dims, block2_dw_weights, &dw_bias_dims, block2_dw_bias,
                                      &block_out_dims, block_buffer,
                                      block2_dw_bn_multiplier, block2_dw_bn_shift);
    if (status != ARM_CMSIS_NN_SUCCESS) goto cleanup;
    status = pointwise_conv2d_bn_relu(&pw_params, &pw_quant, &block_in_dims, block_buffer,
                                      &pw_filter_dims, block2_pw_weights, &pw_bias_dims, block2_pw_bias,
                                      &block_out_dims, conv1_output,
                                      block2_pw_bn_multiplier, block2_pw_bn_shift);
    if (status != ARM_CMSIS_NN_SUCCESS) goto cleanup;

    /* --- Block 3 --- */
    status = depthwise_conv2d_bn_relu(&dw_params, &dw_quant, &block_in_dims, conv1_output,
                                      &dw_filter_dims, block3_dw_weights, &dw_bias_dims, block3_dw_bias,
                                      &block_out_dims, block_buffer,
                                      block3_dw_bn_multiplier, block3_dw_bn_shift);
    if (status != ARM_CMSIS_NN_SUCCESS) goto cleanup;
    status = pointwise_conv2d_bn_relu(&pw_params, &pw_quant, &block_in_dims, block_buffer,
                                      &pw_filter_dims, block3_pw_weights, &pw_bias_dims, block3_pw_bias,
                                      &block_out_dims, conv1_output,
                                      block3_pw_bn_multiplier, block3_pw_bn_shift);
    if (status != ARM_CMSIS_NN_SUCCESS) goto cleanup;

    /* --- Block 4 --- */
    status = depthwise_conv2d_bn_relu(&dw_params, &dw_quant, &block_in_dims, conv1_output,
                                      &dw_filter_dims, block4_dw_weights, &dw_bias_dims, block4_dw_bias,
                                      &block_out_dims, block_buffer,
                                      block4_dw_bn_multiplier, block4_dw_bn_shift);
    if (status != ARM_CMSIS_NN_SUCCESS) goto cleanup;
    status = pointwise_conv2d_bn_relu(&pw_params, &pw_quant, &block_in_dims, block_buffer,
                                      &pw_filter_dims, block4_pw_weights, &pw_bias_dims, block4_pw_bias,
                                      &block_out_dims, conv1_output,
                                      block4_pw_bn_multiplier, block4_pw_bn_shift);
    if (status != ARM_CMSIS_NN_SUCCESS) goto cleanup;

    /* === Global Average Pooling === */
    /* The conv output is of shape [BATCH_SIZE, H, W, BLOCK_OUT_C]. We average over H and W
       to get a 1-D vector of length BLOCK_OUT_C.
    */
    int8_t gap_output[BLOCK_OUT_C];
    global_average_pooling_s8(&block_out_dims, conv1_output, gap_output);

    /* === Fully Connected Layer === */
    cmsis_nn_dims fc_in_dims = { .n = 1, .h = 1, .w = 1, .c = BLOCK_OUT_C };
    cmsis_nn_dims fc_filter_dims = { .n = FC_OUT, .h = 1, .w = 1, .c = BLOCK_OUT_C };
    cmsis_nn_dims fc_bias_dims = { .n = 0, .h = 0, .w = 0, .c = FC_OUT };
    cmsis_nn_dims fc_out_dims = { .n = 1, .h = 1, .w = 1, .c = FC_OUT };

    status = fully_connected_layer(&fc_quant_params, &fc_in_dims, gap_output,
                                     &fc_filter_dims, fc_weights, &fc_bias_dims, fc_bias,
                                     &fc_out_dims, output);
    if (status != ARM_CMSIS_NN_SUCCESS) goto cleanup;

    /* === Softmax === */
    softmax_layer_s8(output, 1, FC_OUT, output);

cleanup:
    free(conv1_output);
    free(block_buffer);
    return status;
}

/*----------------------------------------------------------------------------
  Unit Test Main
 *---------------------------------------------------------------------------*/
int main(int argc, char **argv)
{
    /* 
       For inference the TFLite file name is not used because the weights are compiled in.
       (The tflite file was used by generate_ds_cnn_data.py to produce ds_cnn_data.c.)
    */
    int8_t output[FC_OUT];
    arm_cmsis_nn_status status = ds_cnn_inference(test_input, output);
    if (status != ARM_CMSIS_NN_SUCCESS) {
        printf("DS–CNN inference failed with status %d\n", status);
        return -1;
    }
    
    /* Compare the output with expected_output (both arrays are generated by the Python script) */
    int pass = 1;
    for (int i = 0; i < FC_OUT; i++) {
        if (output[i] != expected_output[i]) {
            pass = 0;
            break;
        }
    }
    if (pass) {
        printf("DS–CNN unit test PASSED.\n");
    } else {
        printf("DS–CNN unit test FAILED.\n");
        printf("Expected: ");
        for (int i = 0; i < FC_OUT; i++)
            printf("%d ", expected_output[i]);
        printf("\nGot: ");
        for (int i = 0; i < FC_OUT; i++)
            printf("%d ", output[i]);
        printf("\n");
    }
    return 0;
}
