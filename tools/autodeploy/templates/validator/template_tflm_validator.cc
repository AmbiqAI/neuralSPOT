/**
 * @file tflm_validator.cc
 * @author Carlos Morales
 * @brief Shell application for instantiating a TFLM model and allowing a RPC
 * client to set input tensors, invoke() it, and collect output tensors
 * @version 0.1
 * @date 2023-02-28
 *
 * @copyright Copyright (c) 2023
 *
 */

#include <cstdlib>
#include <cstring>

#include "mut_model_metadata.h"

#include "tflm_ns_model.h"
#if (TFLM_MODEL_LOCATION == NS_AD_PSRAM) or (TFLM_ARENA_LOCATION == NS_AD_PSRAM)
    #include "ns_peripherals_psram.h"
#endif
// if the model is not in PSRAM
#if (TFLM_MODEL_LOCATION != NS_AD_PSRAM)
    #include "mut_model_data.h"
#endif

#include "tflm_validator.h"
#include "ns_ambiqsuite_harness.h"
#include "ns_core.h"
#include "ns_malloc.h"
#include "ns_peripherals_power.h"
#include "ns_uart.h"
#include "ns_rpc_generic_data.h"
#ifdef AM_PART_APOLLO5B
#include "ns_pmu_utils.h"
#include "ns_pmu_map.h"
#endif

#ifdef AM_PART_APOLLO5B
#define NS_OUTPUT_TENSOR_BUFFER_SIZE 200000
#elif defined(AM_PART_APOLLO3) or defined(AM_PART_APOLLO3P)
#define NS_OUTPUT_TENSOR_BUFFER_SIZE 20000
#else
#define NS_OUTPUT_TENSOR_BUFFER_SIZE 60000
#endif

#define NS_AD_RPC_TRANSPORT_UART 0
#define NS_AD_RPC_TRANSPORT_USB 1

#if (configAPPLICATION_ALLOCATED_HEAP == 1)
size_t ucHeapSize = (NS_RPC_MALLOC_SIZE_IN_K + 8) * 1024;
NS_SRAM_BSS uint8_t ucHeap[(NS_RPC_MALLOC_SIZE_IN_K + 8) * 1024] __attribute__((aligned(4)));
#endif

// TFLM Config and arena
ns_model_state_t tflm;

// TF Tensor Arena

#if (TFLM_MODEL_LOCATION == NS_AD_PSRAM)
    unsigned char *mut_model;
#endif

#if (TFLM_ARENA_LOCATION == NS_AD_PSRAM)
    static uint8_t *tensor_arena;
    static constexpr int kTensorArenaSize = 1024 * 1024 * 10; // 10MB
#else
    static constexpr int kTensorArenaSize = 1024 * TFLM_VALIDATOR_ARENA_SIZE;
    // #ifdef AM_PART_APOLLO3
    //     // Apollo3 doesn't have AM_SHARED_RW
    //     alignas(16) static uint8_t tensor_arena[kTensorArenaSize];
    // #else // not AM_PART_APOLLO3
        #if (TFLM_ARENA_LOCATION == NS_AD_SRAM)
            #ifdef keil6
            // Align to 16 bytes
            NS_SRAM_BSS __attribute__((aligned(16))) static uint8_t tensor_arena[kTensorArenaSize];
            #else
            NS_SRAM_BSS alignas(16) static uint8_t tensor_arena[kTensorArenaSize];
            #endif
        #else
            NS_PUT_IN_TCM alignas(16) static uint8_t tensor_arena[kTensorArenaSize];
        #endif
    // #endif
#endif

// Resource Variable Arena - always in TCM for now
static constexpr int kVarArenaSize =
    4 * (TFLM_VALIDATOR_MAX_RESOURCE_VARIABLES + 1) * sizeof(tflite::MicroResourceVariables);
alignas(16) static uint8_t var_arena[kVarArenaSize];

// Validator Stuff
NS_SRAM_BSS ns_incoming_config_t mut_cfg;
NS_SRAM_BSS ns_outgoing_stats_t mut_stats;
static uint32_t invokes_so_far = 0;
bool input_tensor_is_chunked = false;
bool output_tensor_is_chunked = false;
bool stats_is_chunked = false;
uint32_t input_tensor_offset = 0;
uint32_t output_tensor_offset = 0;
uint32_t model_chunk_offset = 0;
uint32_t stats_offset = 0;
uint32_t stats_remaining = 0;
uint32_t pmu_events_per_layer = 0;

ns_incoming_tensor_details_u inputTensorDetails[NS_MAX_INPUT_TENSORS];
ns_incoming_tensor_details_u outputTensorDetails[NS_MAX_OUTPUT_TENSORS];
extern int tflm_validator_model_init(ns_model_state_t *ms);

#ifdef NS_MLPROFILE
// Timer is used for TF profiling
ns_timer_config_t basic_tickTimer = {
    .api = &ns_timer_V1_0_0,
    .timer = NS_TIMER_COUNTER,
    .enableInterrupt = false,
};

#ifdef AM_PART_APOLLO5B
ns_pmu_config_t pmu_cfg;
#endif

#endif

// PSRAM Stuff
#if (TFLM_MODEL_LOCATION == NS_AD_PSRAM) or (TFLM_ARENA_LOCATION == NS_AD_PSRAM)
ns_psram_config_t psram_cfg = {
    .api = &ns_psram_V0_0_1,
    .psram_enable = true,
    .psram_type = PSRAM_TYPE_HEX,
    // .psram_type = PSRAM_TYPE_OCT,
#ifdef apollo510_evb
    .psram_block = 0,
#else
    .psram_block = 3,
#endif
    .psram_size = 0, // filled in by init
    .psram_base_address = 0, // filled in by init
};
#endif // remote model load

// Automatically generated by the autodeploy tool
#ifdef NS_MLPROFILE
NS_AD_LAYER_METADATA_CODE
const ns_perf_mac_count_t basic_mac = {
    .number_of_layers = tflm_validator_number_of_estimates,
    .mac_count_map = (uint32_t *)tflm_validator_mac_estimates,
    .output_magnitudes = (uint32_t *)NS_AD_NAME_output_magnitudes,
    .stride_h = (uint32_t *)NS_AD_NAME_stride_h,
    .stride_w = (uint32_t *)NS_AD_NAME_stride_w,
    .dilation_h = (uint32_t *)NS_AD_NAME_dilation_h,
    .dilation_w = (uint32_t *)NS_AD_NAME_dilation_w,
    .mac_compute_string = (const char **)NS_AD_NAME_mac_strings,
    .output_shapes = (const char **)NS_AD_NAME_output_shapes,
    .filter_shapes = (const char **)NS_AD_NAME_mac_filter_shapes
};
#endif

/**
 * @brief Initializes the model per config struct
 *
 * @param block - serialized tflm_config_struct
 * @return status
 */
status configureModel(const dataBlock *in) {
    ns_lp_printf("[INFO] PC requested model initialization\n");

    // Grab incoming buffer, decode into config struct
    if (in->buffer.dataLength < sizeof(mut_cfg)) { // Sanity check before memcpy
        ns_lp_printf(
            "[ERROR] Incoming config is too small, expected %d, got %d\n", sizeof(mut_cfg),
            in->buffer.dataLength);
        return ns_rpc_data_failure;
    }

    memcpy(&mut_cfg, in->buffer.data, sizeof(mut_cfg));

    if (in->buffer.dataLength != (sizeof(mut_cfg) + 4 * (mut_cfg.config.num_input_tensors +
                                                         mut_cfg.config.num_output_tensors))) {
        ns_lp_printf(
            "[ERROR] Configuration Size mismatch, expected %d, got %d\n",
            sizeof(mut_cfg) +
                4 * (mut_cfg.config.num_input_tensors + mut_cfg.config.num_output_tensors),
            in->buffer.dataLength);
        return ns_rpc_data_failure;
    }

    ns_lp_printf(
        "[INFO] MUT configuration: profile %d, warmup %d, input tensor length %d, output "
        "tensor length %d, num_input_tensors %d, num_output_tensors %d\n",
        mut_cfg.config.profile_mut, mut_cfg.config.profile_warmup, mut_cfg.config.input_length,
        mut_cfg.config.output_length, mut_cfg.config.num_input_tensors, mut_cfg.config.num_output_tensors);

    // buffer.data contains variable length arrays after the preamble
    // uint32_t inputTensorLengths[numInputTensors]
    // uint32_t outputTensorLengths[numPutputTensors]

    tflm.runtime = TFLM;
    tflm.model_array = mut_model;
    tflm.arena = tensor_arena;
    tflm.arena_size = kTensorArenaSize;
    tflm.rv_arena = var_arena;
    tflm.rv_arena_size = kVarArenaSize;
    tflm.rv_count = TFLM_VALIDATOR_MAX_RESOURCE_VARIABLES;
    tflm.numInputTensors = mut_cfg.config.num_input_tensors;
    tflm.numOutputTensors = mut_cfg.config.num_output_tensors;

    memcpy(
        &inputTensorDetails, in->buffer.data + sizeof(mut_cfg),
        4 * mut_cfg.config.num_input_tensors);
    memcpy(
        &outputTensorDetails,
        in->buffer.data + sizeof(mut_cfg) + (4 * mut_cfg.config.num_input_tensors),
        4 * mut_cfg.config.num_output_tensors);

#ifdef NS_MLPROFILE
    ns_lp_printf("[INFO] Initializing Profiler\n");
    tflm.tickTimer = &basic_tickTimer;
    tflm.mac_estimates = &basic_mac;
    #ifdef AM_PART_APOLLO5B
    tflm.pmu = &pmu_cfg;
    #endif
#else
    tflm.tickTimer = NULL;
#endif
    ns_lp_printf("[INFO] Initializing Model\n");
    int status = tflm_validator_model_init(&tflm);
    ns_lp_printf("[INFO] Model Initialized status = %d\n", status);
    mut_stats.stats.computed_arena_size = tflm.computed_arena_size;
    ns_lp_printf("[INFO] Input Size %d \n", tflm.interpreter->inputs_size());
    ns_lp_printf("[INFO] Output Size %d \n", tflm.interpreter->outputs_size());

    pmu_events_per_layer = 0; // This is set after first stats returned
    ns_lp_printf("[INFO] Full PMU Request %d\n", mut_cfg.config.full_pmu_stats);

    ns_rpc_genericDataOperations_printDatablock(in);
    if (status == 0) {
        return ns_rpc_data_success;
    } else {
        return ns_rpc_data_failure;
    }
}

/**
 * @brief Gets a chunk of tensor via sendBlockToEVB_cb
 *
 * @param in - chunk
 * @return status
 */
status incomingTensorChunk(const dataBlock *in) {
    ns_lp_printf(
        "[INFO] PC Sent Input Tensor Chunk of %d bytes, copied to %d\n", in->buffer.dataLength,
        input_tensor_offset);
    // Get latest chunk, copy into next spot in raw
    // tensor
    memcpy(
        tflm.model_input[0]->data.int8 + input_tensor_offset, in->buffer.data,
        in->buffer.dataLength);
    input_tensor_offset += in->buffer.dataLength;
    input_tensor_is_chunked = true;
    return ns_rpc_data_success;
}

int imc = 0;
/**
 * @brief Gets a chunk of model via sendBlockToEVB_cb
 *
 * @param in - chunk
 * @return status
 */
status incomingModelChunk(const dataBlock *in) {
    // ns_lp_printf(
    //     "[INFO] PC Sent Model Chunk of %d bytes, copied to %d\n", in->buffer.dataLength,
    //     model_chunk_offset);
    ns_lp_printf(".");
    imc++;
    if (imc > 100) {
        ns_lp_printf("\n");
        imc = 0;
    }
    // Get around a compiler error when model is in MRAM (ie. const)
    //
    #if (TFLM_MODEL_LOCATION == NS_AD_MRAM)
        ns_lp_printf("[ERROR] Model in MRAM, cannot write\n");
        return ns_rpc_data_failure;
    #else
        // Get latest chunk, copy into next spot in model array
        memcpy(&mut_model[model_chunk_offset], in->buffer.data, in->buffer.dataLength);
        model_chunk_offset += in->buffer.dataLength;
    #endif
    return ns_rpc_data_success;
}

status decodeIncomingSendblock(const dataBlock *in) {
    if (in->cmd == 0) {
        return configureModel(in);
    } else if (in->cmd == 4) {
        return incomingTensorChunk(in);
    } else {
        return incomingModelChunk(in);
    }
}

uint32_t pmuCounterLayer = 0;

// Handler for fetchBlockFromEVB, invoked by PC
/**
 * @brief Get the statistics about the last Invoke()
 *
 * @param block - serialized tflm_stats_struct
 * @return status
 */
status getStatistics(dataBlock *res) {

    uint32_t statSize = sizeof(mut_stats.bytes) * sizeof(uint8_t);
    uint32_t bufSize = (statSize <= (TFLM_VALIDATOR_RX_BUFSIZE - 1000))
                           ? statSize
                           : (TFLM_VALIDATOR_RX_BUFSIZE - 1000);

    // If the pmu_events_per_layer value isn't 0, this is a special getStatistics that only returns the
    // counterValues for the PMU events for layer pmuCounterLayer
    if (pmu_events_per_layer == 0) {
        ns_lp_printf(
            "[INFO] Server asked for statistics (%d bytes), send back %d bytes\n", statSize, bufSize);
    } else {
        ns_lp_printf(
            "[INFO] Server asked for PMU statistics for layer %d\n", pmuCounterLayer);
        #ifdef AM_PART_APOLLO5B
        bufSize = sizeof(ns_pmu_stats_t);
        #endif
    }

    uint8_t *resultBuffer = (uint8_t *)ns_malloc(bufSize);
    char *msg_store = (char *)ns_malloc(sizeof(char) * 30);
    res->length = bufSize;
    res->dType = uint8_e;
    res->description = msg_store;
    res->cmd = generic_cmd;
    binary_t binaryBlock = {.data = (uint8_t *)resultBuffer, .dataLength = bufSize};
    res->buffer = binaryBlock;

    #if defined(AM_PART_APOLLO3) or defined(AM_PART_APOLLO3P)
    mut_stats.stats.platform = NS_MUT_STATS_PLATFORM_AP3;
    #elif defined(AM_PART_APOLLO4P) or defined (AM_PART_APOLLO4L)
    mut_stats.stats.platform = NS_MUT_STATS_PLATFORM_AP4;
    #elif defined(AM_PART_APOLLO5B)
    mut_stats.stats.platform = NS_MUT_STATS_PLATFORM_AP5;
    #else
    mut_stats.stats.platform = 0;
    #error "Unknown platform"
    #endif

    if (mut_cfg.config.full_pmu_stats == 1) {
        mut_stats.stats.pmu_count = NS_NUM_PMU_MAP_SIZE;
    } else {
        mut_stats.stats.pmu_count = 0;
    }

    mut_stats.stats.computed_stat_buffer_size = sizeof(mut_stats.bytes);
    #ifdef AM_PART_APOLLO5B
    if (pmu_events_per_layer != 0) {
        // bufSize = mut_stats.stats.pmu_events_per_layer * sizeof(uint32_t);
        // mut_stats.stats.computed_stat_per_event_size = bufSize;
        ns_lp_printf("[INFO] Preparing to send %d events, %d bytes for layer %d\n", pmu_events_per_layer, bufSize, pmuCounterLayer);
    } else {
        mut_stats.stats.computed_stat_per_event_size = sizeof(ns_profiler_event_stats_t);
    }
    #else
        mut_stats.stats.computed_stat_per_event_size = sizeof(ns_profiler_event_stats_t);
    #endif // AM_PART_APOLLO5B
    ns_lp_printf("[INFO] computed_stat_per_event_size %d\n", mut_stats.stats.computed_stat_per_event_size);
#ifdef NS_TFLM_VALIDATOR
    if (pmu_events_per_layer == 0) {
        mut_stats.stats.captured_events = ns_microProfilerSidecar.captured_event_num;
        memcpy(
            mut_stats.stats.stat_buffer, ns_profiler_events_stats, sizeof(mut_stats.stats.stat_buffer));

        // copy in the csv header
        memcpy(mut_stats.stats.csv_header, ns_profiler_csv_header, sizeof(mut_stats.stats.csv_header));
        // ns_lp_printf("MUT csv header: %s\n", mut_stats.stats.csv_header);
    }
#else
    mut_stats.stats.captured_events = 0;
#endif

#ifdef AM_PART_APOLLO5B
    if (pmu_events_per_layer != 0) {
        // A non-zero value means we are returning PMU stats for a specific layer
        // We need to fetch the stats for that layer into an array and send it over
        // Parameters are:
        /* uint32_t ns_get_layer_counters(uint32_t layer,
                      uint32_t num_layers,
                      uint32_t rv,
                      uint32_t *out_counters) */
                    
        ns_set_pmu_header();

        memcpy(mut_stats.pmu_stats.csv_header, ns_profiler_pmu_header, sizeof(ns_profiler_pmu_header));
        mut_stats.pmu_stats.layer = pmuCounterLayer;
        ns_get_layer_counters(pmuCounterLayer, 
            TFLM_VALIDATOR_MAC_ESTIMATE_COUNT,
            TFLM_VALIDATOR_MAX_RESOURCE_VARIABLES, 
            mut_stats.pmu_stats.pmu_event_counters);
    } 
#endif

    memcpy(resultBuffer, mut_stats.bytes, bufSize);

    if (pmu_events_per_layer != 0) {
        char msg[] = "FullPMUStats\0";
        pmuCounterLayer++;
        memcpy(msg_store, msg, sizeof(msg));
        // If its the last layer, set pmu_events_per_layer to 0
        if (pmuCounterLayer == TFLM_VALIDATOR_MAC_ESTIMATE_COUNT) {
            pmu_events_per_layer = 0;
            pmuCounterLayer = 0;
        }
    } else if (bufSize == statSize) {
        stats_offset = 0;
        stats_is_chunked = false;
        stats_remaining = 0;
        char msg[] = "FullStats\0";
        memcpy(msg_store, msg, sizeof(msg));

        // After last stats, we need to switch to PMU layer mode if full PMU stats were requested
        if (mut_cfg.config.full_pmu_stats == 1) {
            pmuCounterLayer = 0;
            mut_stats.stats.pmu_events_per_layer = NS_NUM_PMU_MAP_SIZE;
            pmu_events_per_layer = mut_stats.stats.pmu_events_per_layer; // mut_stats is a union, can't use it as state
        }
    } else {
        // We have to chunk the response
        stats_offset = bufSize;
        stats_remaining = statSize - stats_offset;
        stats_is_chunked = true;
        char msg[] = "PartStats\0";
        memcpy(msg_store, msg, sizeof(msg));
    }
    ns_lp_printf(
        "[INFO] Sending stats chunk, sr %d, cs %d, so %d\n", stats_remaining, bufSize,
        stats_offset);
    ns_lp_printf(
        "[INFO] size of rx buffer %d, and tx buffer is %d\n", TFLM_VALIDATOR_RX_BUFSIZE,
        TFLM_VALIDATOR_TX_BUFSIZE);

    // Print mut_stats struct
    // ns_lp_printf("[INFO] Printing mut_stats struct\n");
    // ns_lp_printf("[INFO] computed_arena_size %d\n", mut_stats.stats.computed_arena_size);
    // ns_lp_printf("[INFO] computed_stat_buffer_size %d\n", mut_stats.stats.computed_stat_buffer_size);
    // ns_lp_printf("[INFO] computed_stat_per_event_size %d\n", mut_stats.stats.computed_stat_per_event_size);
    // ns_lp_printf("[INFO] pmu_events_per_layer %d\n", pmu_events_per_layer);
    // ns_lp_printf("[INFO] captured_events %d\n", mut_stats.stats.captured_events);
    // ns_lp_printf("[INFO] platform %d\n", mut_stats.stats.platform);

    // // First 10 bytes of the mut_stats bytes
    // for (int i = 0; i < 10; i++) {
    //     ns_lp_printf("%02x ", mut_stats.bytes[i]);
    // }
    // ns_lp_printf("\n");
    // // First 10 bytes of buffer
    // for (int i = 0; i < 10; i++) {
    //     ns_lp_printf("%02x ", resultBuffer[i]);
    // }
    // ns_lp_printf("\n");

    // ns_rpc_genericDataOperations_printDatablock(res);

    return ns_rpc_data_success;
}

status getStatChunk(dataBlock *res) {
    // Return a chunk of stats

    uint32_t chunkSize = (stats_remaining <= (TFLM_VALIDATOR_RX_BUFSIZE - 1000))
                             ? stats_remaining
                             : (TFLM_VALIDATOR_RX_BUFSIZE - 1000);

    ns_lp_printf(
        "[INFO] Server asked for stats chunk, stats remaining %d, offset %d, sending %d\n",
        stats_remaining, stats_offset, chunkSize);

    uint8_t *resultBuffer = (uint8_t *)ns_malloc(chunkSize);
    char *msg_store = (char *)ns_malloc(sizeof(char) * 30);
    res->length = sizeof(mut_stats.bytes) * sizeof(uint8_t);
    res->dType = uint8_e;
    res->description = msg_store;
    res->cmd = write_cmd;
    binary_t binaryBlock = {.data = (uint8_t *)resultBuffer, .dataLength = chunkSize};
    res->buffer = binaryBlock;

    memcpy(resultBuffer, mut_stats.bytes + stats_offset, chunkSize);
    stats_remaining = stats_remaining - chunkSize;
    if (stats_remaining == 0) {
        stats_is_chunked = false;
        stats_offset = 0;
        char msg[] = "LastStats\0";
        ns_lp_printf("[INFO] Last stats chunk\n");
        memcpy(msg_store, msg, sizeof(msg));
        // After last stats, we need to switch to PMU layer mode if full PMU stats were requested
        if (mut_cfg.config.full_pmu_stats == 1) {
            pmuCounterLayer = 0;
            mut_stats.stats.pmu_events_per_layer = NS_NUM_PMU_MAP_SIZE;
            pmu_events_per_layer = mut_stats.stats.pmu_events_per_layer; // mut_stats is a union, can't use it as state
        }        
    } else {
        stats_offset = stats_offset + chunkSize;
        char msg[] = "PartStats\0";
        ns_lp_printf(
            "[INFO] Part stats chunk sr %d, cs %d, so %d\n", stats_remaining, chunkSize,
            stats_offset);
        memcpy(msg_store, msg, sizeof(msg));
    }

    return ns_rpc_data_success;
}

status decodeIncomingFetchblock(dataBlock *ret) {
    if (stats_is_chunked == false) {
        return getStatistics(ret);
    } else {
        return getStatChunk(ret);
    }
}

// Handler for computeOnEVB, invoked by PC
/**
 * @brief Calls TFLM's invoke()
 *
 * @param in  - input tensor
 * @param res - output tensor
 * @return status - fail if not configured or if invoke fails
 */
uint32_t output_tensor_chunk_offset = 0;
uint32_t totalSize = 0;
uint32_t remaining = 0;
NS_SRAM_BSS uint8_t output_tensor_buffer[NS_OUTPUT_TENSOR_BUFFER_SIZE];
binary_t binaryBlock;

// For PMU characterization
int tf_invoke() {
    return (int) tflm.interpreter->Invoke();
}

bool full_characterization_done = false;

status infer_on_tflm(const dataBlock *in, dataBlock *res) {
    uint32_t outputSize;
    char msg_full[] = "FullTensor\0";
    char msg_part[] = "PartTensor\0";
    char msg_last[] = "LastTensor\0";
    char error_msg[] = "Invoke failed\0";

    uint8_t *resultBuffer;
    char *msg_store;
    ns_lp_printf("[INFO] infer_on_tflm platform %d\n", mut_stats.stats.platform);

    // If the output tensor is greater than 3000, we need to chunk it
    if (mut_cfg.config.output_length < (TFLM_VALIDATOR_RX_BUFSIZE - 1000)) {
        // ns_lp_printf("[INFO] Output tensor is small enough to fit in one block\n");
        output_tensor_is_chunked = false;
        outputSize = mut_cfg.config.output_length;
    } else {
        // ns_lp_printf("[INFO] Output tensor is too large, will chunk\n");
        output_tensor_is_chunked = true;
        outputSize = TFLM_VALIDATOR_RX_BUFSIZE - 1000;
    }

    // Check the command value and if it is 'write' then send an output tensor chunk
    msg_store = (char *)ns_malloc(sizeof(char) * 30);

    if (in->cmd == write_cmd) {
        if (remaining >= TFLM_VALIDATOR_RX_BUFSIZE - 1000) {
            // Send the maximum chunk size
            outputSize = TFLM_VALIDATOR_RX_BUFSIZE - 1000;
        } else {
            // Send the remaining chunk
            outputSize = remaining;
        }
        remaining = remaining - outputSize;
        // ns_lp_printf("[INFO] PC requested input tensor chunk, trying to malloc %d\n", outputSize);
        resultBuffer = (uint8_t *)ns_malloc(outputSize);
        memcpy(resultBuffer, output_tensor_buffer + output_tensor_chunk_offset, outputSize);
        output_tensor_chunk_offset = output_tensor_chunk_offset + outputSize;
        res->description = msg_store;
        res->length = outputSize;
        res->dType = uint8_e;
        if (remaining > 0) {
            memcpy(msg_store, msg_part, sizeof(msg_part));
        } else {
            memcpy(msg_store, msg_last, sizeof(msg_last));
        }
        res->cmd = write_cmd;

        binaryBlock = {
            .data = (uint8_t *)resultBuffer,
            .dataLength = outputSize};
        res->buffer = binaryBlock;
        memcpy(resultBuffer, in->buffer.data, in->buffer.dataLength);
        return ns_rpc_data_success;
    }
    else {
        if (output_tensor_is_chunked) {
            memcpy(msg_store, msg_part, sizeof(msg_part));
        }
        else {
            memcpy(msg_store, msg_full, sizeof(msg_full));
        }
    }

    // Prep the return block, needs to happen whether errors occur or not
    // ns_lp_printf("[INFO] PC requested inference, trying to malloc %d\n", mut_cfg.config.output_length);
    resultBuffer = (uint8_t *)ns_malloc(outputSize);
    res->length = outputSize;
    res->dType = uint8_e;
    res->description = msg_store;
    res->cmd = generic_cmd;
    binaryBlock = {
        .data = (uint8_t *)resultBuffer,
        .dataLength = outputSize};
    res->buffer = binaryBlock;

    // 'in' contains the input tensors, treat as homogeneous block
    if (input_tensor_is_chunked == false) {
        memcpy(tflm.model_input[0]->data.int8, in->buffer.data, in->buffer.dataLength);
    } // else it is already in the input tensor

    TfLiteStatus invoke_status = kTfLiteOk;
    // Run the normal way, collecting only defined stats (for PMU, that is 4 32b event counters)
    ns_lp_printf("[INFO] Invoking model with %d byte tensor\n", in->buffer.dataLength);
    invoke_status = tflm.interpreter->Invoke();
    ns_lp_printf("[INFO] Invoke platform %d\n", mut_stats.stats.platform);

    if (invoke_status != kTfLiteOk) {
        ns_lp_printf("Invoke failed\n");
        memcpy(msg_store, error_msg, sizeof(error_msg));
        return ns_rpc_data_failure;
    }
    ns_lp_printf("[INFO] Invoke successful\n");

    if ((mut_cfg.config.profile_mut == 1) && (invokes_so_far == mut_cfg.config.profile_warmup)) {
        ns_lp_printf(
            "[INFO] requested warmup %d,  invokes_so_far %d", mut_cfg.config.profile_warmup,
            invokes_so_far);
        tflm.profiler->LogCsv(); // prints and also captures events in a buffer
        ns_stop_perf_profiler();
        #ifdef AM_PART_APOLLO5B
        if ((full_characterization_done == false) && (mut_cfg.config.full_pmu_stats == 1)) {
            // Collect the full stats on the first run if requested
            // Note that LogCSV has a the (desirable) side effect of capturing stats for the first run
            // that aren't availabe outside of TFLM, so we let that run before characterizing

            ns_lp_printf("[INFO] Invoking model with full PMU stats\n");
            full_characterization_done = true;
            invoke_status = (TfLiteStatus) ns_characterize_model(tf_invoke);
                ns_lp_printf("[INFO] ns_characterize_model platform %d\n", mut_stats.stats.platform);

        }
        #endif
    }
    ns_lp_printf("[INFO] after profiler successful\n");

    // Prep the return block with output tensor
    // If the output tensor is chunked, we need to copy it to holding buffer
    // and return the first chunk

    // Calculate the total size
    for (uint32_t t = 0; t < mut_cfg.config.num_output_tensors; t++) {
        totalSize += outputTensorDetails[t].details.tensorSizeBytes;
    }
    // ns_lp_printf("[INFO] totalSize %d\n", totalSize);

    // If the output tensor is too big, we need to copy it to holding buffer
    uint8_t *destination = totalSize < (TFLM_VALIDATOR_RX_BUFSIZE - 1000) ? resultBuffer : output_tensor_buffer;

    int offset = 0;
    for (uint32_t t = 0; t < mut_cfg.config.num_output_tensors; t++) {
        memcpy(
            destination + offset, tflm.model_output[t]->data.int8,
            outputTensorDetails[t].details.tensorSizeBytes);
        offset += outputTensorDetails[t].details.tensorSizeBytes;
    }
    // ns_lp_printf("[INFO] output memcpy successful\n");

    // If the output tensor is too big, put the first chunk in the return block
    if (totalSize > (TFLM_VALIDATOR_RX_BUFSIZE - 1000)) {
        memcpy(resultBuffer, destination, outputSize);
        output_tensor_chunk_offset = outputSize;
        remaining = totalSize - outputSize;
    }
    // ns_lp_printf("[INFO] output tensor remaining %i\n", remaining);

    // ns_lp_printf("[INFO] output memcpy successful\n");
    // char res_msg[] = "Invoke Successful!\0";
    ns_lp_printf(".");
    invokes_so_far++;
    input_tensor_offset = 0;
    return ns_rpc_data_success;
}

void ns_preAction(void) { ns_lp_printf("Starting action\n"); }

void ns_postAction(void) { ns_lp_printf("Stopping action\n"); }

NS_SRAM_BSS uint8_t tflm_v_cdc_rx_ff_buf[TFLM_VALIDATOR_RX_BUFSIZE] __attribute__((aligned(4)));
NS_SRAM_BSS uint8_t tlfm_v_cdc_tx_ff_buf[TFLM_VALIDATOR_TX_BUFSIZE] __attribute__((aligned(4)));

int main(void) {
    ns_core_config_t ns_core_cfg = {.api = &ns_core_V1_0_0};

    NS_TRY(ns_core_init(&ns_core_cfg), "Core init failed.\b");
    NS_TRY(ns_power_config(&ns_development_default), "Power Init Failed\n");
    ns_itm_printf_enable();

    ns_lp_printf("sizeof arena %d\n", sizeof(tensor_arena));
    ns_lp_printf("sizeof mut_cfg %d\n", sizeof(mut_cfg));
    ns_lp_printf("sizeof mut_stats %d\n", sizeof(mut_stats));
    ns_lp_printf("sizeof output_tensor_buffer %d\n", sizeof(output_tensor_buffer));
    ns_lp_printf("ns_microProfilerSidecar %d\n", sizeof(ns_microProfilerSidecar));
    ns_lp_printf("ns_profiler_events_stats %d\n", sizeof(ns_profiler_events_stats));
    ns_lp_printf("ns_profiler_csv_header %d\n", sizeof(ns_profiler_csv_header));

#ifdef NS_MLPROFILE
    NS_TRY(ns_timer_init(&basic_tickTimer), "Timer init failed.\n");

#ifdef AM_PART_APOLLO5B
    // PMU config for profiling
    pmu_cfg.api = &ns_pmu_V1_0_0;
    ns_pmu_reset_config(&pmu_cfg);

    // Add events
    ns_pmu_event_create(&pmu_cfg.events[0], NS_PROFILER_PMU_EVENT_0, NS_PMU_EVENT_COUNTER_SIZE_32);
    ns_pmu_event_create(&pmu_cfg.events[1], NS_PROFILER_PMU_EVENT_1, NS_PMU_EVENT_COUNTER_SIZE_32);
    ns_pmu_event_create(&pmu_cfg.events[2], NS_PROFILER_PMU_EVENT_2, NS_PMU_EVENT_COUNTER_SIZE_32);
    ns_pmu_event_create(&pmu_cfg.events[3], NS_PROFILER_PMU_EVENT_3, NS_PMU_EVENT_COUNTER_SIZE_32);   
    ns_pmu_init(&pmu_cfg); // PMU config passed to model init, which passes it to debugLogInit
#endif
#endif


#if (TFLM_MODEL_LOCATION == NS_AD_PSRAM) or (TFLM_ARENA_LOCATION == NS_AD_PSRAM)
    NS_TRY(ns_psram_init(&psram_cfg), "PSRAM Init Failed\n");
#endif

#if (TFLM_MODEL_LOCATION == NS_AD_PSRAM)
    // mut_model = (unsigned char *)MSPI_XIP_BASE_ADDRESS;
    mut_model = (unsigned char *)(psram_cfg.psram_base_address);
#endif

#if (TFLM_ARENA_LOCATION == NS_AD_PSRAM)
    // tensor_arena = (uint8_t *)MSPI_XIP_BASE_ADDRESS + (1024*1024*20); // leave 20MB for model
    tensor_arena = (unsigned char *)(psram_cfg.psram_base_address) + (1024*1024*20);

    // tensor_arena = (uint8_t *)mut_model + (1024*1024*20); // leave 20MB for model
#endif

    // ns_lp_printf("Model Address: 0x%x, arena 0x%x\n", mut_model, tensor_arena);

    ns_interrupt_master_enable();

    #if (NS_VALIDATOR_RPC_TRANSPORT == NS_AD_RPC_TRANSPORT_UART)
    ns_uart_config_t rpcGenericUARTHandle = {
        .api = &ns_uart_V0_0_1,
        .uart_config = NULL,
        .rx_cb = NULL,
        .tx_cb = NULL,
        .tx_blocking = true,
        .rx_blocking = true};
    #endif

    // Add callbacks to handle incoming requests
    ns_rpc_config_t rpcConfig = {
        .api = &ns_rpc_gdo_V1_1_0,
        .mode = NS_RPC_GENERICDATA_SERVER, // Puts EVB in RPC server mode
        .rx_buf = tflm_v_cdc_rx_ff_buf,
        .rx_bufLength = TFLM_VALIDATOR_RX_BUFSIZE,
        .tx_buf = tlfm_v_cdc_tx_ff_buf,
        .tx_bufLength = TFLM_VALIDATOR_TX_BUFSIZE,
    #if (NS_VALIDATOR_RPC_TRANSPORT == NS_AD_RPC_TRANSPORT_UART)
        .uartHandle = (ns_uart_handle_t)&rpcGenericUARTHandle,
    #else
        .uartHandle = NULL,
    #endif
        .sendBlockToEVB_cb = decodeIncomingSendblock,
        .fetchBlockFromEVB_cb = decodeIncomingFetchblock,
        .computeOnEVB_cb = infer_on_tflm,
    #if (NS_VALIDATOR_RPC_TRANSPORT == NS_AD_RPC_TRANSPORT_UART)
        .transport = NS_RPC_TRANSPORT_UART
    #else
        .transport = NS_RPC_TRANSPORT_USB
    #endif
    };
    NS_TRY(ns_rpc_genericDataOperations_init(&rpcConfig), "RPC Init Failed\n");

    // Add some pre/post callbacks
    // erpc_server_add_pre_cb_action(&ns_preAction);
    // erpc_server_add_post_cb_action(&ns_postAction);

    ns_lp_printf("Ready to receive RPC Calls\n");
    ns_lp_printf(
        "Debug kVarArenaSize %d, var_arena %d, resource variables %d\n", kVarArenaSize, sizeof(var_arena),
        sizeof(tflite::MicroResourceVariables));
    while (1) {
        ns_rpc_genericDataOperations_pollServer(&rpcConfig);
        ns_delay_us(1000);
        // ns_deep_sleep();
    }
}
