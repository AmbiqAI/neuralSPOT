# Automatically Generated Model Static Library
This directory was generated by neuralSPOT Autodeploy and includes:
1. A static library (and related headers) containing the model
2. The source code used to generate this static library
3. A minimal example that compiles the model into a binary deployable on Apollo platforms
4. A makefile that ties it all together

```
lib\
  modelname_api.h   # API for interacting with the model
  modelname_lib.a   # Static lib encapsulating the model
  ns_model.h        # API for model wrapper
  tensorflow\
    lib\            # platform specific TFLM static libs
    directories containing tensorflow headers...

src\
  # Code that is compiled into modelname_lib.a
  modelname_model.cc       # Init, allocate, and invoke methods for the model
  modelname_model.h        # Model constants
  modelname_model_data.h   # Model weights

  # Code used to generate deployable Apollo binary
  modelname_example.cc         # Initializes the model and invokes it once
  modelname_example_tensors.h  # Randomized input and output tensors
  ns-core\                     # startup and linker scripts
  CMSIS\                       # Apollo system config
  system_apolloX.c             # Apollo helper functions
```

  ## Using the Model Lib
  The example file illustrates the minimal steps needed to initialize and
  invoke the model.

  ```c
    int status = mobilenetv3_minimal_init(&model); // model init with minimal defaults

    // At this point, the model is ready to use - init and allocations were successful
    // Note that the model handle is not meant to be opaque, the structure is defined
    // in ns_model.h, and contains state, config details, and model structure information

    // Get data about input and output tensors
    int numInputs = model.numInputTensors;
    int numOutputs = model.numOutputTensors;

    // Initialize input tensors
    int offset = 0;
    for (int i = 0; i < numInputs; i++) {
        memcpy(model.model_input[i]->data.int8, ((char *)mobilenetv3_example_input_tensors) + offset,
               model.model_input[i]->bytes);
        offset += model.model_input[i]->bytes;
    }

    // Execute the model
    TfLiteStatus invoke_status = model.interpreter->Invoke();

    // Compare the bytes of the output tensors against expected values
    offset = 0;
    for (int i = 0; i < numOutputs; i++) {
        if (0 != memcmp(model.model_output[i]->data.int8,
                        ((char *)mobilenetv3_example_output_tensors) + offset,
                        model.model_output[i]->bytes)) {
            while (1)
                example_status = mobilenetv3_STATUS_INVALID_CONFIG; // miscompare, so hang
        }
        offset += model.model_output[i]->bytes;
    }

    while (1) {
        // Success!
        example_status = mobilenetv3_STATUS_SUCCESS;
    }
```

## Choosing TFLM and AmbiqSuite Versions
Autodeploy copies needed files from TFLM and Ambiqsuite (TFLM headers and libs, AmbiqSuite CMSIS). To change the default versions used for this, use the following Autodeploy parameters:

```bash
  --ambiqsuite-version AMBIQSUITE_VERSION
                        AmbiqSuite version used to generate minimal example (default: Apollo510_SDK3_2024_09_14)
  --tensorflow-version TENSORFLOW_VERSION
                        Tensorflow version used to generate minimal example (default: Oct_08_2024_e86d97b6)
```
